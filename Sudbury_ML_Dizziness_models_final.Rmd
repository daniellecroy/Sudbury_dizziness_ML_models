---
title: "Machine Learning Dizziness Models"
output: html_document
date: "2025-06-04"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE)
```


```{r, results="hide"}
#Load Packages
Packages <- c("readxl", "tree", "dplyr", "rpart", "MLmetrics", "caret", "xgboost", "rBayesianOptimization", "mltools", "data.table", "randomForest", "performanceEstimation", "ROSE", "gbm", "caret", "glmnet", "epiR", "table1", "pROC", "ROCR")
lapply(Packages, library, character.only=TRUE)

#Import data
train<-suppressMessages(read_xlsx(path="~/Library/Mobile Documents/com~apple~CloudDocs/Research/HSNRI - Research/Ohle - BPPV/Stroke Risk Score/Ohlescorederivation_June2024.xlsx", sheet=1))
test<-suppressMessages(read_xlsx(path="~/Library/Mobile Documents/com~apple~CloudDocs/Research/HSNRI - Research/Ohle - BPPV/Stroke Risk Score/Ohlescorevalidation_June2024.xlsx", sheet=1))
```

**Data Cleaning**
```{r}
#Data Cleaning
#Change outcome to factor variable
train<-train%>% mutate(outcome= case_when(compevent>0.9 ~ "yes", TRUE ~ "no"))
test<-test%>% mutate(outcome= case_when(compevent>0.9 ~ "yes", TRUE ~ "no"))
train$outcome <- as.factor(train$outcome)
test$outcome <- as.factor(test$outcome)

table(train$compevent, train$outcome) #make sure correct reclassification
table(test$compevent, test$outcome) #make sure correct reclassification

#Keep only necessary variables
train_subset<- train %>% dplyr::select(c(patient_ID, bppv, timing_long_more_than_2_min:multiple, trigger_head_turning:dysphagia, diplopia, nausea:recent_viral_urti_symptoms, diabetes, atrial_fibrillation, previous_stroke, motor:spontaneous_or_gaze_evoked_nysta, age, sexe, pre_tia2, any_neuro_deficits, diage:hypert, outcome))
test_subset<- test %>% dplyr::select(c(patient_ID, bppv, timing_long_more_than_2_min:multiple, trigger_head_turning:dysphagia, diplopia, nausea:recent_viral_urti_symptoms, diabetes, atrial_fibrillation, previous_stroke, motor:spontaneous_or_gaze_evoked_nysta, age, sexe, pre_tia2, any_neuro_deficits, diage:hypert, outcome, ct_performed, referred))

#Convert character variables into factors
col_names <- names(train_subset)
train_subset[,col_names] <- lapply(train_subset[,col_names], factor)
col_names <- names(test_subset)
test_subset[,col_names] <- lapply(test_subset[,col_names], factor)

#Merge train and 60% of the test set
set.seed(1215)
size<-floor(0.7*nrow(test_subset))
train_test_ind<-sample(seq_len(nrow(test_subset)),size=size)
train_60<-test_subset[train_test_ind,]
test_60_<-test_subset[-train_test_ind,]
train_full_<-full_join(train_subset,train_60)

train_full<-subset(train_full_, select=-c(trigger_head_turning, trigger_getting_up, trigger_lying_down, trigger_bending_over, trigger_looking_up, trigger_rolling_over_in_bed, trigger_walking, trigger_any_movements, trigger_persistent_symptoms_when, nausea, vomitting, headache, neck_pain_discomfort, facial_eye_pain, hearing_loss, tinnitus, recent_viral_urti_symptoms, unable_to_walk_unaided, can_walk_more_than_10_steps, spontaneous_or_gaze_evoked_nysta, ongoing, gradual, abrupt, timing_long_more_than_2_min, ct_performed, referred))

test_60<-subset(test_60_, select=-c(trigger_head_turning, trigger_getting_up, trigger_lying_down, trigger_bending_over, trigger_looking_up, trigger_rolling_over_in_bed, trigger_walking, trigger_any_movements, trigger_persistent_symptoms_when, nausea, vomitting, headache, neck_pain_discomfort, facial_eye_pain, hearing_loss, tinnitus, recent_viral_urti_symptoms, unable_to_walk_unaided, can_walk_more_than_10_steps, spontaneous_or_gaze_evoked_nysta, ongoing, gradual, abrupt, timing_long_more_than_2_min, ct_performed, referred))
```


**Descriptive statistics**
```{r}
#Get a full clean unbalanced dataset (merging both train and test sets)
train_full2<-train_full
train_full2$dev_val<-"Train"
test_60_2<-test_60
test_60_2$dev_val<-"Test"
full_dizzy<-full_join(train_full2,test_60_2)

full_dizzy2<-subset(full_dizzy, select=c(patient_ID, bppv, single, multiple, dysphagia, diplopia, diabetes, atrial_fibrillation, previous_stroke, motor, objective_sensory, ataxia, dysarthria, dysmetria, age, sexe, pre_tia2, hypert, outcome, dev_val))
full_dizzy2$age2<-as.numeric(as.character(full_dizzy2$age))
full_dizzy2$bppv<-factor(full_dizzy2$bppv, levels=c("no","yes"), labels=c("No", "Yes"))

#Create labels and levels
label(full_dizzy2$bppv)<-"BPPV"
full_dizzy2$single<-factor(full_dizzy2$single, levels=c("no","yes"), labels=c("No", "Yes"))
label(full_dizzy2$single)<-"Single episode"
full_dizzy2$multiple<-factor(full_dizzy2$multiple, levels=c("no","yes"), labels=c("No", "Yes"))
label(full_dizzy2$multiple)<-"Multiple episodes"
full_dizzy2$dysphagia<-factor(full_dizzy2$dysphagia, levels=c("no","yes"), labels=c("No", "Yes"))
label(full_dizzy2$dysphagia)<-"Dysphagia"
full_dizzy2$diplopia<-factor(full_dizzy2$diplopia, levels=c("no","yes"), labels=c("No", "Yes"))
label(full_dizzy2$diplopia)<-"Diplopia"
full_dizzy2$diabetes<-factor(full_dizzy2$diabetes, levels=c("no","yes"), labels=c("No", "Yes"))
label(full_dizzy2$diabetes)<-"Diabetes"
full_dizzy2$atrial_fibrillation<-factor(full_dizzy2$atrial_fibrillation, levels=c("n","y"), labels=c("No", "Yes"))
label(full_dizzy2$atrial_fibrillation)<-"Atrial fibrillation"
full_dizzy2$previous_stroke<-factor(full_dizzy2$previous_stroke, levels=c("no","yes"), labels=c("No", "Yes"))
label(full_dizzy2$previous_stroke)<-"Previous stroke"
full_dizzy2$motor<-factor(full_dizzy2$motor, levels=c("no","yes"), labels=c("No", "Yes"))
label(full_dizzy2$motor)<-"Motor deficits"
full_dizzy2$objective_sensory<-factor(full_dizzy2$objective_sensory, levels=c("no","yes"), labels=c("No", "Yes"))
label(full_dizzy2$objective_sensory)<-"Objective sensory deficits"
full_dizzy2$ataxia<-factor(full_dizzy2$ataxia, levels=c("no","yes"), labels=c("No", "Yes"))
label(full_dizzy2$ataxia)<-"Ataxia"
full_dizzy2$dysarthria<-factor(full_dizzy2$dysarthria, levels=c("no","yes"), labels=c("No", "Yes"))
label(full_dizzy2$dysarthria)<-"Dysarthria"
full_dizzy2$dysmetria<-factor(full_dizzy2$dysmetria, levels=c("no","yes"), labels=c("No", "Yes"))
label(full_dizzy2$dysmetria)<-"Dysmetria"
full_dizzy2$sexe<-factor(full_dizzy2$sexe, levels=c("F","M"), labels=c("Female", "Male"))
label(full_dizzy2$sexe)<-"Biological sex"
label(full_dizzy2$age2)<-"Age"
full_dizzy2$pre_tia2<-factor(full_dizzy2$pre_tia2, levels=c("n","y"), labels=c("No", "Yes"))
label(full_dizzy2$pre_tia2)<-"Previous TIA"
full_dizzy2$hypert<-factor(full_dizzy2$hypert, levels=c(0,1), labels=c("No", "Yes"))
label(full_dizzy2$hypert)<-"Hypertension"
full_dizzy2$outcome<-factor(full_dizzy2$outcome, levels=c("no","yes"), labels=c("No", "Yes"))
label(full_dizzy2$outcome)<-"Serious Diagnosis"
full_dizzy2$dev_val <- factor(full_dizzy2$dev_val, 
                              levels = c("Train", "Test"))

#Rendering functions to get correct number of decimals in table1's
my.render.cont <- function(x) {
    with(stats.apply.rounding(stats.default(x), digits=4), c("",
        "Mean (SD)"=sprintf("%s (&plusmn; %s)", MEAN, SD)))
}
my.render.cat <- function(x) {
    c("", sapply(stats.default(x), function(y) with(y,
        sprintf("%d (%0.1f %%)", FREQ, PCT))))
}

table1<-(table1(~ age2+sexe+hypert+diabetes+atrial_fibrillation+previous_stroke+pre_tia2+bppv+single+multiple+diplopia+dysphagia+dysmetria+dysarthria+ataxia+motor+objective_sensory | outcome, data=full_dizzy2, render.categorical=my.render.cat, c(left="Overall")))
table1
readr::write_lines(table1, "vertigo_ML_table1.html", append=F)

table2<-(table1(~ age2+sexe+hypert+diabetes+atrial_fibrillation+previous_stroke+pre_tia2+bppv+single+multiple+diplopia+dysphagia+dysmetria+dysarthria+ataxia+motor+objective_sensory | dev_val, data=full_dizzy2, render.categorical=my.render.cat, c(left="Overall")))
table2
readr::write_lines(table2, "vertigo_ML_table2.html", append=F)
```

**Balancing Data**
```{r}
set.seed(360)
#Apply over- and under-sampling
df_train <- 
  ovun.sample(outcome ~. -diage -patient_ID -any_neuro_deficits, data=train_full, p=0.5, seed=360, method="both")$data
```



**Random Forest**
```{r}
#RANDOM FOREST MODEL
set.seed(1200)

#Determine best mtry
train_control<-trainControl(method="cv",number=5,search="grid")

tune_grid <- expand.grid(.mtry=c(3,4,5))

set.seed(1200)
rf_model<-train(outcome ~. -diage -patient_ID -any_neuro_deficits, data=df_train, method="rf", trControl=train_control, tuneGrid=tune_grid)
print(rf_model) #mtry=5 produced best result

#Build dataframes for analyses
df_train_temp<-df_train
df_train_temp$age<-as.numeric(as.character(df_train$age))
df_predictors_train<-subset(df_train_temp, select=-c(diage, patient_ID, any_neuro_deficits))
test_60$age<-as.numeric(as.character(test_60$age))

#Train RF model
rf_riskscore<-randomForest(outcome~. , data=df_predictors_train, importance=TRUE, ntree=100, keep.inbag=TRUE, mtry=5, cutoff=c(0.95,0.05))
importance_rf<-importance(rf_riskscore)
saveRDS(rf_riskscore, "RF_SeriousDiagnosis_Model.rds")

#Apply predictions to the test data
p2<-predict(rf_riskscore,newdata=test_60, type="prob")
predicted_class<-ifelse(p2[,"yes"]>0.20, "yes", "no")
confusionMatrix(as.factor(predicted_class), as.factor(test_60$outcome), positive="yes")
rf_data<-as.table(matrix(c(30,187,0,1151), nrow=2, byrow=T))
epi.tests(rf_data, conf.level=0.95)

#Assess resource utilization
table(as.factor(predicted_class))
table(as.factor(test_60_$ct_performed))
table(as.factor(test_60_$referred))


#Get variable importance and MDA
importance_df <- as.data.frame(importance(rf_riskscore, type = 1))  # type = 1 is MDA
importance_df$Variable <- rownames(importance_df)
name_map <- c(
  "single" = "Single Episode",
  "multiple" = "Multiple Episodes",
  "dysphagia" = "Dysphagia",
  "diplopia" = "Diplopia",
  "diabetes" = "Diabetes",
  "atrial_fibrillation" = "Atrial Fibrillation",
  "previous_stroke" = "Previous Stroke",
  "pre_tia2" = "Previous TIA",
  "motor" = "Motor Deficits",
  "objective_sensory" = "Sensory Deficits",
  "ataxia" = "Ataxia",
  "dysarthria" = "Dysarthria",
  "dysmetria" = "Dysmetria",
  "hypert" = "Hypertension",
  "sexe" = "Sex",
  "bppv"="BPPV",
  "age"="Age"
)

# Apply mapping
importance_df$Variable <- name_map[importance_df$Variable]
imp_1<-randomForest::importance(rf_riskscore, type = 1, class="yes")[,1]
imp_0<-randomForest::importance(rf_riskscore, type = 1, class="no")[,1]
importance_df$Class0_MDA<-imp_0
importance_df$Class1_MDA<-imp_1
importance_df$betterclass<-ifelse(imp_0>imp_1, "No", "Yes")

importance_df$MeanDecreaseAccuracy_raw<-importance_df$MeanDecreaseAccuracy/100
plot1<-ggplot(importance_df, aes(x=reorder(Variable, MeanDecreaseAccuracy_raw), y=MeanDecreaseAccuracy_raw))+
  geom_point(size=3)+
  coord_flip()+
  labs(x = "Variable", y = "Mean Decrease in Accuracy", title="A)") +
  theme_minimal()+
   theme(
    plot.title = element_text(hjust = 0, size = 14, face = "bold"),
    axis.title=element_text(size=14),
    axis.text.y=element_text(size=10),
    axis.text.x=element_text(size=8)
  )
plot1
  
#Check 2x2 accuracy and resource utilization in full dataset
train_data<-subset(train_full, select=-c(any_neuro_deficits, diage, patient_ID))
test_data<-subset(test_60, select=-c(any_neuro_deficits,diage ,patient_ID))
train_data$age<-as.numeric(as.character(train_data$age))
combined_df <- rbind(train_data, test_data)

p_full1<-predict(rf_riskscore, newdata=combined_df, type="prob")
predict_full<-ifelse(p_full1[,"yes"]>0.20, "yes", "no")
confusionMatrix(as.factor(predict_full), as.factor(combined_df$outcome), positive="yes")

rf_full_data<-as.table(matrix(c(215,755,2,5667), nrow=2, byrow=T))
epi.tests(rf_full_data, conf.level=0.95)

train_full_$age<-as.numeric(as.character(train_full_$age))
resource_full<-rbind(train_full_, test_60_)
table(as.factor(predict_full))
table(as.factor(resource_full$ct_performed))
table(as.factor(resource_full$referred))
```

**XGBoost Model**
```{r, warning=FALSE}
##Need to hot one-encode the training data for xgboost model
df_train_xgb<-df_train
df_train_xgb$bppv<-ifelse(df_train$bppv=="yes",1,0)
df_train_xgb$single<-ifelse(df_train$single=="yes",1,0)
df_train_xgb$multiple<-ifelse(df_train$multiple=="yes",1,0)
df_train_xgb$dysphagia<-ifelse(df_train$dysphagia=="yes",1,0)
df_train_xgb$diplopia<-ifelse(df_train$diplopia=="yes",1,0)
df_train_xgb$diabetes<-ifelse(df_train$diabetes=="yes",1,0)
df_train_xgb$atrial_fibrillation<-ifelse(df_train$atrial_fibrillation=="y",1,0)
df_train_xgb$motor<-ifelse(df_train$motor=="yes",1,0)
df_train_xgb$objective_sensory<-ifelse(df_train$objective_sensory=="yes",1,0)
df_train_xgb$ataxia<-ifelse(df_train$ataxia=="yes",1,0)
df_train_xgb$dysarthria<-ifelse(df_train$dysarthria=="yes",1,0)
df_train_xgb$dysmetria<-ifelse(df_train$dysmetria=="yes",1,0)
df_train_xgb$sexeM<-ifelse(df_train$sexe=="M",1,0)
df_train_xgb$pre_tia2<-ifelse(df_train$pre_tia2=="y",1,0)
df_train_xgb$diage<-as.numeric(df_train$diage)-1
df_train_xgb$hypert<-as.numeric(df_train$hypert)-1
df_train_xgb$previous_stroke<-ifelse(df_train$previous_stroke=="yes",1,0)
df_train_xgb$outcome_xgb<-ifelse(df_train$outcome=="yes",1,0)
df_train_xgb$age<-as.numeric(as.character(df_train$age))
df_train_xgb<-subset(df_train_xgb, select=-c(patient_ID, any_neuro_deficits, diage, sexe, outcome))

#Need to hot one-encode the test data too
df_test_xgb<-test_60
df_test_xgb$bppv<-ifelse(test_60$bppv=="yes",1,0)
df_test_xgb$single<-ifelse(test_60$single=="yes",1,0)
df_test_xgb$multiple<-ifelse(test_60$multiple=="yes",1,0)
df_test_xgb$dysphagia<-ifelse(test_60$dysphagia=="yes",1,0)
df_test_xgb$diplopia<-ifelse(test_60$diplopia=="yes",1,0)
df_test_xgb$diabetes<-ifelse(test_60$diabetes=="yes",1,0)
df_test_xgb$atrial_fibrillation<-ifelse(test_60$atrial_fibrillation=="y",1,0)
df_test_xgb$motor<-ifelse(test_60$motor=="yes",1,0)
df_test_xgb$objective_sensory<-ifelse(test_60$objective_sensory=="yes",1,0)
df_test_xgb$ataxia<-ifelse(test_60$ataxia=="yes",1,0)
df_test_xgb$dysarthria<-ifelse(test_60$dysarthria=="yes",1,0)
df_test_xgb$dysmetria<-ifelse(test_60$dysmetria=="yes",1,0)
df_test_xgb$sexeM<-ifelse(test_60$sexe=="M",1,0)
df_test_xgb$pre_tia2<-ifelse(test_60$pre_tia2=="y",1,0)
df_test_xgb$hypert<-as.numeric(test_60$hypert)-1
df_test_xgb$previous_stroke<-ifelse(test_60$previous_stroke=="yes",1,0)
df_test_xgb$outcome_xgb<-ifelse(test_60$outcome=="yes",1,0)
df_test_xgb$age<-as.numeric(as.character(df_test_xgb$age))
df_test_xgb<-subset(df_test_xgb, select=-c(patient_ID, any_neuro_deficits, sexe, outcome, diage))

#Convert the training dataframes into a matrices
df_train_xgb2 <- subset(df_train_xgb, select=-outcome_xgb)
colnames(df_train_xgb2)<-c("BPPV", "Single episode", "Multiple episodes", "Dysphagia", "Diplopia", "Diabetes", "Atrial fibrillation", "Previous stroke", "Motor deficits", "Objective sensory deficits", "Ataxia", "Dysarthria", "Dysmetria", "Age", "Previous TIA", "Hypertension", "Sex")
dt <- data.matrix(df_train_xgb2)
lbl <- as.numeric(df_train_xgb$outcome_xgb)
xgbmatrix <-xgb.DMatrix(data=as.matrix(dt), label=as.numeric(df_train_xgb$outcome_xgb))

#Define training control
train_control2<-trainControl(method="cv",number=5,search="grid")

#Define the grid of hyperparameters
tune_grid2<-expand.grid(
  nrounds=c(50,100),
  eta=c(0.01,0.1),
  max_depth=c(3,5),
  gamma=c(2,5),
  subsample=c(0.5,0.8),
  colsample_bytree=c(0.5,0.8),
  min_child_weight=c(1,5)
)

#Train the model with hyperparameter tuning
set.seed(700) #746
capture.output({
  suppressWarnings({
    xgb_model <- train(
      x = as.matrix(df_train_xgb2),
      y = as.factor(df_train_xgb$outcome_xgb),
      method = "xgbTree",
      trControl = train_control2,
      tuneGrid = tune_grid2,
      metric = "Accuracy"
    )
  })
}, file = ifelse(.Platform$OS.type == "windows", "NUL", "/dev/null"))

#Train model with optimal hyperparameters
bst <- xgboost(data= xgbmatrix, max.depth= 5, eta= 0.1, nrounds= 100, gamma=2, colsample_bytree=0.8, min_child_weight=1, subsample=0.8, objective= "binary:logistic", verbose = 1)

#Convert the testing dataframes into a matrices
df_test_xgb2 <- subset(df_test_xgb, select=-outcome_xgb)
colnames(df_test_xgb2)<-c("BPPV", "Single episode", "Multiple episodes", "Dysphagia", "Diplopia", "Diabetes", "Atrial fibrillation", "Previous stroke", "Motor deficits", "Objective sensory deficits", "Ataxia", "Dysarthria", "Dysmetria", "Age", "Previous TIA", "Hypertension", "Sex")
dt_test <- data.matrix(df_test_xgb2)
lbl <- as.numeric(df_test_xgb$outcome_xgb)
xgbmatrix_test <-xgb.DMatrix(data=dt_test, label=df_test_xgb$outcome_xgb)

#Apply predictions to the test data
p1<-predict(bst,xgbmatrix_test, type="prob")
pred <- ifelse(p1>0.20, 1, 0)
confusionMatrix(as.factor(pred), as.factor(df_test_xgb$outcome_xgb), positive="1")
gb_data<-as.table(matrix(c(29,174,1,1164), nrow=2, byrow=T))
epi.tests(gb_data, conf.level=0.95)

#Assess resource utilization
table(as.factor(pred))
table(as.factor(test_60_$ct_performed))
table(as.factor(test_60_$referred))

#Get variable importance and MDA
X_val<-dt_test
y_val<-df_test_xgb$outcome_xgb
                             
p<-predict(bst,xgbmatrix_test, type="prob")
majority <- ifelse(p> 0.5, 1, 0)

baseline_acc<-Accuracy(majority, y_val)

get_permutation<-function(model, X_val, y_val, baseline_acc){
  importance=numeric(ncol(X_val))
  colnames<-colnames(X_val)
 
   for (i in seq_along(importance)) {
    X_perm <- X_val
    X_perm[, i] <- sample(X_perm[, i])  # Permute column
    preds <- predict(model, newdata = X_perm)
    preds_class <- ifelse(preds > 0.5, 1, 0)
    acc <- Accuracy(preds_class, y_val)
    importance[i] <- baseline_acc - acc  # Decrease in accuracy
  }

  importance_df <- data.frame(Variable = colnames,MeanDecreaseAccuracy = importance)

  importance_df[order(importance_df$MeanDecreaseAccuracy, decreasing = TRUE), ]
}

importance_xgb<-get_permutation(bst, X_val, y_val, baseline_acc)

plot2<-ggplot(importance_xgb, aes(x = reorder(Variable, MeanDecreaseAccuracy), y = MeanDecreaseAccuracy)) +
  geom_point(size = 3) +
  coord_flip() +
  labs(x = "Variable", y = "Mean Decrease in Accuracy", title="B)") +
  theme_minimal()+
   theme(
    plot.title = element_text(hjust = 0, size = 14, face = "bold"),
    axis.title=element_text(size=14),
    axis.text.y=element_text(size=10),
    axis.text.x=element_text(size=8)
  )
plot2

#Check 2x2 accuracy and resource utilization in full dataset
df_full_xgb<-combined_df
df_full_xgb$bppv<-ifelse(df_full_xgb$bppv=="yes",1,0)
df_full_xgb$single<-ifelse(df_full_xgb$single=="yes",1,0)
df_full_xgb$multiple<-ifelse(df_full_xgb$multiple=="yes",1,0)
df_full_xgb$dysphagia<-ifelse(df_full_xgb$dysphagia=="yes",1,0)
df_full_xgb$diplopia<-ifelse(df_full_xgb$diplopia=="yes",1,0)
df_full_xgb$diabetes<-ifelse(df_full_xgb$diabetes=="yes",1,0)
df_full_xgb$atrial_fibrillation<-ifelse(df_full_xgb$atrial_fibrillation=="y",1,0)
df_full_xgb$motor<-ifelse(df_full_xgb$motor=="yes",1,0)
df_full_xgb$objective_sensory<-ifelse(df_full_xgb$objective_sensory=="yes",1,0)
df_full_xgb$ataxia<-ifelse(df_full_xgb$ataxia=="yes",1,0)
df_full_xgb$dysarthria<-ifelse(df_full_xgb$dysarthria=="yes",1,0)
df_full_xgb$dysmetria<-ifelse(df_full_xgb$dysmetria=="yes",1,0)
df_full_xgb$sexeM<-ifelse(df_full_xgb$sexe=="M",1,0)
df_full_xgb$pre_tia2<-ifelse(df_full_xgb$pre_tia2=="y",1,0)
df_full_xgb$hypert<-as.numeric(df_full_xgb$hypert)-1
df_full_xgb$previous_stroke<-ifelse(df_full_xgb$previous_stroke=="yes",1,0)
df_full_xgb$outcome_xgb<-ifelse(df_full_xgb$outcome=="yes",1,0)

df_full_xgb2 <- subset(df_full_xgb, select=-c(outcome_xgb, outcome, sexe))
colnames(df_full_xgb2)<-c("BPPV", "Single episode", "Multiple episodes", "Dysphagia", "Diplopia", "Diabetes", "Atrial fibrillation", "Previous stroke", "Motor deficits", "Objective sensory deficits", "Ataxia", "Dysarthria", "Dysmetria", "Age", "Previous TIA", "Hypertension", "Sex")
dt_full <- data.matrix(df_full_xgb2)

xgbmatrix_full <-xgb.DMatrix(data=dt_full, label=df_full_xgb$outcome_xgb)
pfull<-predict(bst,xgbmatrix_full, type="prob")
pred_xgb_full <- ifelse(pfull>0.20, 1, 0)
confusionMatrix(as.factor(pred_xgb_full), as.factor(df_full_xgb$outcome_xgb), positive="1")
gb_full_data<-as.table(matrix(c(214,795,1,5627), nrow=2, byrow=T))
epi.tests(gb_full_data, conf.level=0.95)
table(as.factor(pred_xgb_full))
table(as.factor(resource_full$ct_performed))
table(as.factor(resource_full$referred))
```


**LASSO Model**
```{r}
#Find best lambda value for LASSO Logistic regression model
lasso_grid<-expand.grid(alpha=1, lambda=seq(0.01,0.1,length.out=10))
train_control3<-trainControl(method="cv", number=5, search="grid")
lasso_model<-train(x=as.matrix(dt), y=as.factor(df_train_xgb$outcome_xgb), method="glmnet", tuneGrid=lasso_grid, trControl=train_control3)

#Train LASSO Logistic regression model with best lambda value
lasso_model_final<-glmnet(x=as.matrix(dt), y=df_train_xgb$outcome_xgb, alpha=1, lambda=0.01, family="binomial")

#Appply predictions to test data
y_pred<-predict(lasso_model_final, newx=dt_test, type="response")
y_pred2 <- ifelse(y_pred>0.20, 1, 0)
confusionMatrix(as.factor(y_pred2), as.factor(df_test_xgb$outcome_xgb), positive="1")
lasso_data<-as.table(matrix(c(30,346,0,992), nrow=2, byrow=T))
lasso_data

#Assess resource utilization
epi.tests(lasso_data, conf.level=0.95)
table(as.factor(y_pred2))
table(as.factor(test_60_$ct_performed))
table(as.factor(test_60_$referred))

#Check 2x2 accuracy and resource utilization in full dataset
y_full_pred<-predict(lasso_model_final, newx=dt_full, type="response")
y_pred_full <- ifelse(y_full_pred>0.20, 1, 0)
confusionMatrix(as.factor(y_pred_full), as.factor(df_full_xgb$outcome_xgb), positive="1")
lasso_full_data<-as.table(matrix(c(214,1531,1,4891), nrow=2, byrow=T))
lasso_full_data
epi.tests(lasso_full_data, conf.level=0.95)
table(as.factor(y_pred_full))
table(as.factor(resource_full$ct_performed))
table(as.factor(resource_full$referred))
```

**Decision tree model**
```{r}
set.seed(234)

#Perform CP grid search
grid<-expand.grid(  
  cp = c(0.01,0.05, 0.1, 0.2))
ctrl<-trainControl(method="cv", number=5, classProbs=T, summaryFunction=twoClassSummary)

#Refactor variables
predictors<-as.data.frame(dt)
predictors$BPPV<-as.factor(predictors$BPPV)
predictors$`Single episode`<-as.factor(predictors$`Single episode`)
predictors$`Multiple episodes`<-as.factor(predictors$`Multiple episodes`)
predictors$Dysphagia<-as.factor(predictors$Dysphagia)
predictors$Diplopia<-as.factor(predictors$Diplopia)
predictors$Diabetes<-as.factor(predictors$Diabetes)
predictors$`Atrial fibrillation`<-as.factor(predictors$`Atrial fibrillation`)
predictors$`Previous stroke`<-as.factor(predictors$`Previous stroke`)
predictors$`Motor deficits`<-as.factor(predictors$`Motor deficits`)
predictors$`Objective sensory deficits`<-as.factor(predictors$`Objective sensory deficits`)
predictors$Ataxia<-as.factor(predictors$Ataxia)
predictors$Dysarthria<-as.factor(predictors$Dysarthria)
predictors$Dysmetria<-as.factor(predictors$Dysmetria)
predictors$`Previous TIA`<-as.factor(predictors$`Previous TIA`)
predictors$Hypertension<-as.factor(predictors$Hypertension)
predictors$Sex<-as.factor(predictors$Sex)

#Rename variables
predictors <- predictors %>%
   dplyr::rename(Single_episode = `Single episode`,
          Multiple_episode= `Multiple episodes`,
          Atrial_fibrillation=`Atrial fibrillation`,
          Previous_stroke=`Previous stroke`,
          Previous_tia=`Previous TIA`,
          motor=`Motor deficits`,
          objective=`Objective sensory deficits`)

#Include outcome and variables in one dataframe
outcome<-as.factor(df_train_xgb$outcome_xgb)
total<-cbind(predictors, outcome)
levels(total$outcome) <- c("Class0", "Class1")

#Build decision tree
tree_model<-train(outcome~BPPV+Single_episode+Multiple_episode+Dysphagia+Diplopia+Diabetes+Atrial_fibrillation+Previous_stroke+Previous_tia+motor+objective+Ataxia+Dysarthria+Dysmetria+Hypertension+Sex, data=total, method="rpart", trControl=ctrl, tuneGrid=grid, metric="ROC")
print(tree_model$bestTune)
plot(tree_model)

#Repeat same reformatting for test data
predictors_test<-as.data.frame(dt_test)
predictors_test$BPPV<-as.factor(predictors_test$BPPV)
predictors_test$`Single episode`<-as.factor(predictors_test$`Single episode`)
predictors_test$`Multiple episodes`<-as.factor(predictors_test$`Multiple episodes`)
predictors_test$Dysphagia<-as.factor(predictors_test$Dysphagia)
predictors_test$Diplopia<-as.factor(predictors_test$Diplopia)
predictors_test$Diabetes<-as.factor(predictors_test$Diabetes)
predictors_test$`Atrial fibrillation`<-as.factor(predictors_test$`Atrial fibrillation`)
predictors_test$`Previous stroke`<-as.factor(predictors_test$`Previous stroke`)
predictors_test$`Motor deficits`<-as.factor(predictors_test$`Motor deficits`)
predictors_test$`Objective sensory deficits`<-as.factor(predictors_test$`Objective sensory deficits`)
predictors_test$Ataxia<-as.factor(predictors_test$Ataxia)
predictors_test$Dysarthria<-as.factor(predictors_test$Dysarthria)
predictors_test$Dysmetria<-as.factor(predictors_test$Dysmetria)
predictors_test$`Previous TIA`<-as.factor(predictors_test$`Previous TIA`)
predictors_test$Hypertension<-as.factor(predictors_test$Hypertension)
predictors_test$Sex<-as.factor(predictors_test$Sex)

#Rename variables
predictors_test <- predictors_test %>%
   dplyr::rename(Single_episode = `Single episode`,
          Multiple_episode= `Multiple episodes`,
          Atrial_fibrillation=`Atrial fibrillation`,
          Previous_stroke=`Previous stroke`,
          Previous_tia=`Previous TIA`,
          motor=`Motor deficits`,
          objective=`Objective sensory deficits`)

#Include outcome and variables in one dataframe
outcome<-as.factor(df_test_xgb$outcome_xgb)
total_test<-cbind(predictors_test, outcome)
levels(total_test$outcome) <- c("Class0", "Class1")

#Apply predictions to test data
tree_pred1<-predict(tree_model, newdata=total_test, type="raw")
confusionMatrix(as.factor(tree_pred1), as.factor(total_test$outcome), positive="Class1")
tree_pred_prob<-predict(tree_model, newdata=total_test, type="prob")
tree_prob_class1<-tree_pred_prob[,"Class1"]
tree_pred_thresh<-ifelse(tree_prob_class1>0.2, 1, 0)
confusionMatrix(as.factor(tree_pred_thresh), as.factor(df_test_xgb$outcome_xgb), positive="1")
tree_data<-as.table(matrix(c(29,210,1,1128), nrow=2, byrow=T))
epi.tests(tree_data, conf.level=0.95)

#Plot decision tree
library(rpart.plot)
tree_final<-tree_model$finalModel
custom_labels<-c(BPPV = "BPPV",
  Single_episode = "Single Episode",
  Multiple_episode = "Multiple Episodes",
  Dysphagia = "Dysphagia",
  Diplopia = "Diplopia",
  Diabetes = "Diabetes",
  Atrial_fibrillation = "Atrial Fibrillation",
  Previous_stroke = "Previous Stroke",
  Previous_tia = "Previous TIA",
  motor = "Motor Deficits",
  objective = "Objective or Sensory Deficits",
  Ataxia = "Ataxia",
  Dysarthria = "Dysarthria",
  Dysmetria = "Dysmetria",
  Hypertension = "Hypertension",
  Sex = "Sex")

tree_final$frame$var <- recode(
  tree_final$frame$var,
  !!!custom_labels,
  .default = tree_final$frame$var
)

rpart.plot(tree_final,
           type = 3,          
           extra = 104,       
           under = TRUE,       
           fallen.leaves = TRUE,
           cex = 0.8,          
           box.palette = "RdYlGn",  
           shadow.col = "gray",
           varlen=0,
           faclen=0)
tree_pred1_<-ifelse(tree_pred1=="Class1",1,0)

#Assess resource utilization
table(as.factor(tree_pred1_), as.factor(test_60_$ct_performed))
table(as.factor(tree_pred1_), as.factor(test_60_$referred))

#Check 2x2 accuracy and resource utilization in full dataset
predictors_full<-as.data.frame(dt_full)
predictors_full$BPPV<-as.factor(predictors_full$BPPV)
predictors_full$`Single episode`<-as.factor(predictors_full$`Single episode`)
predictors_full$`Multiple episodes`<-as.factor(predictors_full$`Multiple episodes`)
predictors_full$Dysphagia<-as.factor(predictors_full$Dysphagia)
predictors_full$Diplopia<-as.factor(predictors_full$Diplopia)
predictors_full$Diabetes<-as.factor(predictors_full$Diabetes)
predictors_full$`Atrial fibrillation`<-as.factor(predictors_full$`Atrial fibrillation`)
predictors_full$`Previous stroke`<-as.factor(predictors_full$`Previous stroke`)
predictors_full$`Motor deficits`<-as.factor(predictors_full$`Motor deficits`)
predictors_full$`Objective sensory deficits`<-as.factor(predictors_full$`Objective sensory deficits`)
predictors_full$Ataxia<-as.factor(predictors_full$Ataxia)
predictors_full$Dysarthria<-as.factor(predictors_full$Dysarthria)
predictors_full$Dysmetria<-as.factor(predictors_full$Dysmetria)
predictors_full$`Previous TIA`<-as.factor(predictors_full$`Previous TIA`)
predictors_full$Hypertension<-as.factor(predictors_full$Hypertension)
predictors_full$Sex<-as.factor(predictors_full$Sex)

predictors_full <- predictors_full %>%
   dplyr::rename(Single_episode = `Single episode`,
          Multiple_episode= `Multiple episodes`,
          Atrial_fibrillation=`Atrial fibrillation`,
          Previous_stroke=`Previous stroke`,
          Previous_tia=`Previous TIA`,
          motor=`Motor deficits`,
          objective=`Objective sensory deficits`)

outcome<-as.factor(df_full_xgb$outcome_xgb)
total_full<-cbind(predictors_full, outcome)
levels(total_full$outcome) <- c("Class0", "Class1")

tree_pred_full<-predict(tree_model, newdata=total_full, type="raw")
confusionMatrix(as.factor(tree_pred_full), as.factor(total_full$outcome), positive="Class1")

tree_pred_prob_full<-predict(tree_model, newdata=total_full, type="prob")
tree_prob_class1_full<-tree_pred_prob_full[,"Class1"]
tree_pred_thresh_full<-ifelse(tree_prob_class1_full>0.2, 1, 0)
confusionMatrix(as.factor(tree_pred_thresh_full), as.factor(df_full_xgb$outcome_xgb), positive="1")
tree_data_full<-as.table(matrix(c(206,896,9,5526), nrow=2, byrow=T))
tree_data_full
epi.tests(tree_data_full, conf.level=0.95)
table(as.factor(tree_pred_thresh_full))
table(as.factor(resource_full$ct_performed))
table(as.factor(resource_full$referred))
```

**CT/Referrals**
```{r}
#Input number of CTs and referrals to. calculate 95% CIs
number_cts_or_refs=503
number_using_preds=239
prop.test(x = number_cts_or_refs-number_using_preds, n = number_cts_or_refs, correct = FALSE)
```

**Sudbury Vertigo Risk Score**
```{r}
#Calculate Sudbury Vertigo Risk Score
test_score<-test_60
test_score$outcome2<-ifelse(test_score$outcome=="yes",1,0)
test_score$temp1<-ifelse(test_score$sexe=="M",1,0)
test_score$temp2<-ifelse(test_score$diage==1,1,0) 
test_score$temp3<-ifelse(test_score$hypert==1,3,0)
test_score$temp4<-ifelse(test_score$motor=="yes"| test_score$objective_sensory=="yes", 5, 0)
test_score$temp5<-ifelse(test_score$ataxia=="yes"| test_score$diplopia=="yes"| test_score$dysarthria=="yes"| test_score$dysphagia=="yes"| test_score$dysmetria=="yes", 6, 0)
test_score$temp6<-ifelse(test_score$bppv=="yes",-5,0)
test_score$sudburyscore<-test_score$temp1+test_score$temp2+test_score$temp3+test_score$temp4+test_score$temp5+test_score$temp6

#Dichotomize score
test_score$sudburyscore_over4<-ifelse(test_score$sudburyscore>4,1,0)
test_score$sudburyscore_over8<-ifelse(test_score$sudburyscore>8,1,0)

#Check performance and accuracy
confusionMatrix(as.factor(test_score$sudburyscore_over4), as.factor(test_score$outcome2), positive="1")
score_data<-as.table(matrix(c(30,357,0,981), nrow=2, byrow=T))
score_data
epi.tests(score_data, conf.level=0.95)

#Assess resource utilization
table(test_60_$ct_performed)
table(test_60_$referred)

#Calculate prediction for AUC curve
riskscore_model1<-glm(outcome2~sudburyscore, data=test_score, family=binomial)
prob=predict(riskscore_model1, type=c("response"))

#Check 2x2 accuracy and resource utilization in full dataset
train_full$age<-as.numeric(as.character(train_full$age))
combined<-rbind(train_full, test_60)
full_score<-combined
full_score$outcome2<-ifelse(full_score$outcome=="yes",1,0)
full_score$temp1<-ifelse(full_score$sexe=="M",1,0)
full_score$temp2<-ifelse(full_score$diage==1,1,0) 
full_score$temp3<-ifelse(full_score$hypert==1,3,0)
full_score$temp4<-ifelse(full_score$motor=="yes" | full_score$objective_sensory=="yes", 5, 0)
full_score$temp5<-ifelse(full_score$ataxia=="yes" | full_score$diplopia=="yes" | full_score$dysarthria=="yes" | full_score$dysphagia=="yes" | full_score$dysmetria=="yes", 6, 0)
full_score$temp6<-ifelse(full_score$bppv=="yes",-5,0)
full_score$sudburyscore<-full_score$temp1+full_score$temp2+full_score$temp3+full_score$temp4+full_score$temp5+full_score$temp6

full_score$sudburyscore_over4<-ifelse(full_score$sudburyscore>4,1,0)
full_score$sudburyscore_over8<-ifelse(full_score$sudburyscore>8,1,0)

confusionMatrix(as.factor(full_score$sudburyscore_over4), as.factor(full_score$outcome2), positive="1")
score_data_full<-as.table(matrix(c(213,1565,2,4857), nrow=2, byrow=T))
score_data_full
epi.tests(score_data_full, conf.level=0.95)
table(as.factor(full_score$sudburyscore_over4))
```

**ROC Curves**
```{r}
#Get predicted probabilities for all the models
tree_pred<-tree_prob_class1
rf_pred<-predict(rf_riskscore,newdata=test_60, type="prob")[,2]
xgb_pred<-predict(bst,xgbmatrix_test)
lasso_pred<-predict(lasso_model_final, newx=dt_test, type="response")
riskscore_pred=predict(riskscore_model1, type=c("response"))

#Create outcome variable in environment
outcome<-ifelse(test_60$outcome=="yes",1,0)

#Calculate AUC and 95% CIs
library(pROC)

##Decision tree
roc_tree<-roc(outcome, tree_pred)
roc_tree
original_auc=cvAUC::AUC(predictions=tree_pred, labels=outcome)
roc<-pROC::roc(response = outcome, predictor = tree_pred)
original_roc_ci<-ci.auc(roc, confidence=0.95, method="bootstrap")
original_roc_ci[1]
original_roc_ci[3]

##RF
roc_rf<-roc(outcome, rf_pred)
roc_rf
original_auc=cvAUC::AUC(predictions=rf_pred, labels=outcome)
roc<-pROC::roc(response = outcome, predictor = rf_pred)
original_roc_ci<-ci.auc(roc, confidence=0.95, method="bootstrap")
original_roc_ci[1]
original_roc_ci[3]

##XGBoost
roc_xgb<-roc(outcome, xgb_pred)
roc_xgb
original_auc=cvAUC::AUC(predictions=xgb_pred, labels=outcome)
roc<-pROC::roc(response = outcome, predictor = xgb_pred)
original_roc_ci<-ci.auc(roc, confidence=0.95, method="bootstrap")
original_roc_ci[1]
original_roc_ci[3]

##LR with LASSO
roc_lasso<-roc(outcome, lasso_pred)
roc_lasso
original_auc=cvAUC::AUC(predictions=lasso_pred, labels=outcome)
roc<-pROC::roc(response = outcome, predictor = lasso_pred)
original_roc_ci<-ci.auc(roc, confidence=0.95, method="bootstrap")
original_roc_ci[1]
original_roc_ci[3]

##Sudbury Vertigo Risk Score
roc_riskscore<-roc(outcome,riskscore_pred)
roc_riskscore
original_auc=cvAUC::AUC(predictions=riskscore_pred, labels=outcome)
roc<-pROC::roc(response = outcome, predictor = riskscore_pred)
original_roc_ci<-ci.auc(roc, confidence=0.95, method="bootstrap")
original_roc_ci[1]
original_roc_ci[3]

#Format data and merge together to construct ROC plot
roc_tree_df<-data.frame(Specificity=rev(roc_tree$specificities),
                        Sensitivity=rev(roc_tree$sensitivities),
                        Model="Decision Tree")
roc_lasso_df<-data.frame(Specificity=rev(roc_lasso$specificities),
                         Sensitivity=rev(roc_lasso$sensitivities),
                         Model="Logistic Regression with LASSO")
roc_rf_df<-data.frame(Specificity=rev(roc_rf$specificities),
                      Sensitivity=rev(roc_rf$sensitivities),
                      Model="Random Forest")
roc_xgb_df<-data.frame(Specificity=rev(roc_xgb$specificities),
                       Sensitivity=rev(roc_xgb$sensitivities),
                       Model="XGBoost")
roc_riskscore_df<-data.frame(Specificity=rev(roc_riskscore$specificities), Sensitivity=rev(roc_riskscore$sensitivities), Model="Sudbury Score")
roc_df<-rbind(roc_tree_df, roc_lasso_df, roc_rf_df, roc_xgb_df, roc_lasso_df, roc_riskscore_df)

# Set desired order for models in the legend and color mapping
roc_df$Model <- factor(roc_df$Model, levels = c(
  "Decision Tree",
  "Logistic Regression with LASSO",
  "Random Forest",
  "XGBoost",
  "Sudbury Score"
))

ggplot(roc_df, aes(x=(1-Specificity), y=Sensitivity, color=Model)) +geom_line() +labs(title="ROC Curves Comparing Different Models", x="1 - Specificity", y="Sensitivity") + theme_minimal()+ coord_fixed(ratio=1)
```

**Brier Scores**
```{r}
#Calculate Brier score and 95% CI
##Decision tree
tree_brierscore=DescTools::BrierScore(outcome,tree_pred)
brier<-BuyseTest::brier(outcome,tree_pred, conf.level = 0.95)
tree_brierscore
brier$lower
brier$upper

##Random forest
rf_brierscore=DescTools::BrierScore(outcome,rf_pred)
brier<-BuyseTest::brier(outcome,rf_pred, conf.level = 0.95)
rf_brierscore
brier$lower
brier$upper

##XGBoost
xgb_brierscore=DescTools::BrierScore(outcome,xgb_pred)
brier<-BuyseTest::brier(outcome,xgb_pred, conf.level = 0.95)
xgb_brierscore
brier$lower
brier$upper

##LR with LASSO
lasso_brierscore=DescTools::BrierScore(outcome,lasso_pred)
brier<-BuyseTest::brier(outcome,lasso_pred, conf.level = 0.95)
lasso_brierscore
brier$lower
brier$upper

##Sudbury Vertigo Risk score
riskscore_brierscore=DescTools::BrierScore(outcome,riskscore_pred)
brier<-BuyseTest::brier(outcome,riskscore_pred, conf.level = 0.95)
riskscore_brierscore
brier$lower
brier$upper
```

```{r, include=F}
############################ Calibration plot #################################
calibration_data1<-data.frame(observed=outcome, predicted=tree_pred, Model="Decision Tree")
calibration_data3<-data.frame(observed=outcome, predicted=lasso_pred, Model="Logistic Regression with LASSO")
calibration_data3$predicted<-calibration_data3$s0
calibration_data3<-calibration_data3[,-2]
calibration_data4<-data.frame(observed=outcome, predicted=rf_pred, Model="Random Forest")
calibration_data5<-data.frame(observed=outcome, predicted=xgb_pred, Model="XGBoost")
calibration_data6<-data.frame(observed=outcome, predicted=riskscore_pred, Model="Sudbury Score")


#Show calibration plot using smoothing
calibration_data <- rbind(calibration_data1, calibration_data3, calibration_data4, calibration_data5, calibration_data6)

calibration_data$Model <- factor(calibration_data$Model, levels = c(
  "Decision Tree",
  "Logistic Regression",
  "Logistic Regression with LASSO",
  "Random Forest",
  "XGBoost",
  "Sudbury Score"
))

ggplot(data = calibration_data, aes(x = predicted, y = observed, color=Model)) +
  geom_smooth(method="lm", se=F, span=2, size=0.5)+       
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "black") + 
  labs(x = "Predicted", y = "Observed") +
  xlim(0, 1) + ylim(0, 1) +
  theme_minimal() + 
  theme(panel.grid = element_blank(), axis.line = element_line(color = "black"))

library(dplyr)
# Bin predictions into quantiles (e.g., deciles)
calibration_binned <- calibration_data %>%
  mutate(bin = ntile(predicted, 10)) %>%
  group_by(Model, bin) %>%
  summarise(
    mean_predicted = mean(predicted),
    mean_observed = mean(observed),
    .groups = "drop"
  )

# Plot using geom_point() or geom_line()
ggplot(data = calibration_binned, aes(x = mean_predicted, y = mean_observed, color = Model)) +
  geom_point(size = 2) +
  geom_line() + 
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "black") +
  labs(x = "Mean Predicted Probability", y = "Mean Observed Frequency") +
  xlim(0, 1) + ylim(0, 1) +
  theme_minimal() +
  theme(panel.grid = element_blank(), axis.line = element_line(color = "black"))
```


**Importance Plots**
```{r, fig.width=12, fig.height=5}
#MDA plots in one plot
library(patchwork)
plot1+plot2
```
